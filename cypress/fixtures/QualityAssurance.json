[
  {
    "QAParameters":["1. Business Goal Comprehension","2. Project Technical Comprehension"],
    "QAQuestions": ["1.1 Does the QA team know the end-user of the application and their motivations for using the application under test?",
      "1.2 Does the QA team understand the customer profiles/upstream-downstream systems identified for the application?",
      "1.3 (If applicable) Does the QA team have access to usability studies and UX documents to test the application from a customer's POV?",
      "1.4 Does the QA team understand the business objectives of the application under test?",
      "1.5 Can the QA team articulate the performance requirements for the application?",
      "1.6 Can the QA team articulate the security requirements for the application",
      "1.7 (If Applicable) Can the QA team articulate the accessibility requirements for the application?",
      "1.8 Does the team have access to critical stakeholders to clarify questions about the requirements for the application?",
      "2.1 Is the technical architecture of the project well understood by the QA team?",
      "2.2 Does the QA team understand the technical decisions taken to decide on the current tech stack?",
      "2.3 Is the QA team aware of tech-stack-specific issues that could culminate (functional/non-functional)",
      "2.4 Is the QA team aware of design best practices employed by the team (development)?",
      "2.5 Are the upstream/downstream dependencies of the application understood by the team?"
    ],
    "notes": [" QA team know the end-user of the application and their motivations for using the application under test",
      "QA team understand the customer profiles/upstream-downstream systems identified for the application",
      "QA team have access to usability studies and UX documents to test the application from a customer's POV",
      "QA team understand the business objectives of the application under test",
      "QA team articulate the performance requirements for the application",
      "QA team articulate the security requirements for the application",
      "team have access to critical stakeholders to clarify questions about the requirements for the application",
      "technical architecture of the project well understood by the QA team",
      "QA team understand the technical decisions taken to decide on the current tech stack",
      "QA team aware of tech-stack-specific issues that could culminate (functional/non-functional)",
      "QA team aware of design best practices employed by the team (development)",
      "upstream/downstream dependencies of the application understood by the team"],
    "QAAssignmentMaturityScoreDesc": ["In addition, the team fully understands the various types of users and their goals with the product/platform. The team understands the competitor products in this space. They are in a position to collaborate with the product team and share feedback on product features.",
      "In addition, the team fully understands the NFRs (security, accessibility, performance requirements.) The team has access to the stakeholders to understand the product goals and functionality.",
      "The team has a good understanding of the overall business/product vision, understand the functional areas, and how they are aligned with the overall business goals.",
      "The team has minimal understanding of the functionality. The team understands isolated functional areas, but not how those are aligned with the overall business/product vision. There is minimal understanding of the users and their needs.",
      "The awareness within the team is minimal or nil.",
      "Team fully understands the architecture design, and provides feedback to improve/optimise architecture",
      "The team understands architecture well and testing driven by known strengths/weaknesses.",
      "The team understands architecture and incorporates some validation based on architecture",
      "The QAs (QA team) understand the tech stack used, but aren't aware of the architecture or design",
      "The QAs (QA team) aren't aware of the technical details and architecture of the product/platform"
    ]
  },
  {
    "TSParameters":["1. Test Preparedness","2. Sensible Defaults"],
    "TSQuestions": ["1.1 Is the team aware of existing tech debt and the impact it will have on the quality of the project?",
      "1.2 Is the team aware of the existing team structure and how teams are supposed to work together?",
      "1.3 Is there a test strategy created for the application under test?",
      "2.1 Does the test strategy include functional and cross-functional testing attributes?",
      "2.2 Does the test strategy call out assumptions and limitations for the testing to be done?",
      "2.3 Does the test strategy include sensible defaults for requirements and story creation?",
      "2.4 Does the test strategy include sensible defaults for the testing process to be followed by QA on teams? (Standups, Dev box, IPMs, Showcases, etc.)",
      "2.5 Does the test strategy include sensible defaults for identifying automation workflows?",
      "2.6 Does the test strategy include sensible defaults for identifying automation tools?",
      "2.7 Does the test strategy include sensible defaults for best practices for framework creation using the above tools?",
      "2.8 Does the test strategy include sensible default templates for test cases to be created?",
      "2.9 Does the test strategy include sensible defaults for reporting tests?",
      "2.10 Does the test strategy include sensible defaults for test case management?",
      "2.11 Does the test strategy include sensible defaults for reporting?",
      "2.12 Does the test strategy include details on sensible defaults for Smoke testing?",
      "2.13 Does the test strategy include details on sensible defaults for CICD integration?",
      "2.14 Does the test strategy include sensible defaults for running tests on the pipeline?",
      "2.15 Does the test strategy include sensible defaults for extensibility and test suite maintenance?",
      "2.16 Does the test strategy include sensible defaults for defect management?",
      "2.17 Does the test strategy include sensible defaults for handling change management?",
      "2.18 Does the test strategy include sensible defaults for handling regression testing?",
      "2.19 Does the test strategy include sensible defaults for performance testing?",
      "2.20 Does the test strategy include sensible defaults for security testing?",
      "2.21 Does the test strategy include sensible defaults for integration testing?",
      "2.22 Does the test strategy include sensible defaults for communication between teams ?"
    ],
    "notes": [" QA team know the end-user of the application and their motivations for using the application under test",
      "QA team understand the customer profiles/upstream-downstream systems identified for the application",
      "QA team have access to usability studies and UX documents to test the application from a customer's POV",
      "QA team understand the business objectives of the application under test",
      "QA team articulate the performance requirements for the application",
      "QA team articulate the security requirements for the application",
      "team have access to critical stakeholders to clarify questions about the requirements for the application",
      "technical architecture of the project well understood by the QA team",
      "QA team understand the technical decisions taken to decide on the current tech stack",
      "QA team aware of tech-stack-specific issues that could culminate (functional/non-functional)",
      "QA team aware of design best practices employed by the team (development)",
      "upstream/downstream dependencies of the application understood by the team"],
    "TSAssignmentMaturityScoreDesc": ["The entire development team understands the testing strategy and contributes to updating based on the implementation experience. The strategy is exhaustive and covers the breadth, but at the same time simple to understand, implement and update.",
      "Test strategy is a living document, and is updated quite regularly based on implementation feedback. The team is well versed with the latest version, and the implementation seldom deviates from the strategy.",
      "The test strategy document is understood fairly among the various members of the QA team and is followed for most of the execution. Updates to the testing strategy based on implementation feedback are minimal and take longer time.",
      "Test strategy document exists, but is not implemented or updated regularly. The understanding is fragmented across the various team members.",
      "No test strategy exists",
      "Best of industry and pioneering approaches used across the team",
      "Sensible defaults are listed comprehensively and followed regularly. Updates are done in a timely manner, based on the alignment to product/platform and technical requirements.",
      "Key sensible defaults are listed and implemented.",
      "Only a few sensible defaults are listed, and followed inconsistently",
      "There are no sensible defaults defined or implemented in the team"

    ]
  },
  {
    "TOSParameters":["1. Test Process","2. Metrics","3. Test Management"],
    "TOSQuestions": ["1.1 Is the QA Team an active participant in daily standups?",
      "1.2 Does the QA Team clarify requirements/acceptance criteria when a story is created?",
      "1.3 Does the QA Team perform dev box testing? (intermittent testing does not count)",
      "1.4 Does the QA Team review dev unit test cases for correctness and completeness?",
      "1.5 Does the QA Team flag and follow up on bad quality practices by team members?",
      "1.6 Does the QA Team participate actively in showcases to display work completed?",
      "1.7 Does the QA Team ensure inputs for the QA effort required for stories get included in the total effort?",
      "1.8 Does the QA Team set aside the effort and time needed to develop automation?",
      "1.9 Does the team keep an updated track of test cases (does not need to be detailed) by the component that is visible to everyone (Confluence pages etc.)?",
      "1.10 Does the team tag stories/bugs tested by test cases?",
      "1.11 Does the team indicate which tests need to be automated and which need to be manual? This needs to be updated regularly",
      "1.12 Does the team have a smoke test suite?",
      "1.13 Does the QA Team track effort required to automate tests and ensure it is included in the total effort of development?",
      "1.14 Do test cases get updated immediately if story requirements are changed?",
      "1.15 Does the team update the tests to be performed based on the power of a test? (Only high-power tests - tests most likely to catch a bug - should be in the test list. All low-power tests should be culled)",
      "1.16 Does the team organize bug bashes before a release?",
      "1.17 Does the team perform root cause analysis on critical/high-severity defects?",
      "1.18 Does the team conduct/participate in retrospectives?",
      "1.19 Does the team have an onboarding process for new joinees?",
      "2.1 Does the team use quality metrics/indices to keep a track of quality by component which then builds up to the overall quality of the application?",
      "2.2 Does the team track bugs by priority and severity?",
      "2.3 Does the team track automation and code coverage metrics?",
      "3.1 Does the team use the test management software?",
      "3.2 Does the team use the test management software for defect management?",
      "3.3 Does the team use the test management software to drive testing efforts?",
      "3.4 Does the team use the test management software to track automation?",
      "3.5 Does the team use the test management software to track test drives?",
      "3.6 Does the team use the test management software to track quality metrics?"
    ],
    "TOSAssignmentMaturityScoreDesc": ["Additionally, the team is in a position to take decisions on expectations for the process where needed.",
      "The testing process is defined, and well-understood across the team. The process is followed for all testing and modified where and when necessary.",
      "The testing process is defined and documented. The majority of the team understands the testing process, and the team follows the defined process for a majority of the stories/epics.",
      "The testing process is partly defined. The team is not fully onboard with the process, and intermittently follows the defined process.",
      "The testing process isn't defined, and testing flow across stories/epics is inconsistent",
      "Metrics tracked truly indicate the quality of the testing and software. Trends in metrics are tracked regularly, and inform the key decisions the team needs to take to improve the software and testing.",
      "The team tracks key metrics that inform the quality of testing implemented. The metric values are a good indication of how the testing is performed, and quality of the software.",
      "The team has some useful metrics and they track them regularly. Metrics are an afterthought.",
      "The team tracks a few activity metrics. The metrics are sometimes dysfunctional and aren't a true reflection of the team's performance or testing quality.",
      "The team doesn't use any metrics to drive the testing.",
      "Test management is completely up to date, and all tests are run through the test management tools. There is a good record of all the tests run, and the corresponding results. Tracing older results is easy and common.",
      "All tests are run through test management, and the tests are up to date with minor exceptions.",
      "Test management tools are used by the team regularly. There are some gaps in the overall tests, automation and documentation.",
      "Test management software is in place but not used by the team for test efforts consistently. The tests and other documentation are out of date.",
      "There is no test management software or practice in place. The knowledge of tests, automation, and defects isn't maintained in a commonplace."
    ]
  },
  {
    "TPParameters":["1. Unit Testing","2. Functional Testing","3. Automation"],
    "TPQuestions": ["1.1  Does the team perform dev boxes with the developers after development?",
      "1.2  Does the team use code coverage tools?",
      "1.3  Does the team perform code reviews of the unit test code?",
      "2.1  Does the team manage the list of test cases/suites effectively?",
      "2.2  Do test cases get updated regularly as requirements change?",
      "2.3  Does the team use test data effectively?",
      "2.4  Test data is used and maintained effectively",
      "2.5  Has the team created test scenarios to cover integration testing?",
      "2.6  Has the team created test scenarios to cover UAT?",
      "2.7  Team regularly refreshes test data by scenario",
      "2.8  Team regularly refreshes test data by environment",
      "2.9  Team refreshes test suite to remove low-power tests",
      "2.10  Team records the results of test runs in a central report",
      "2.11  Team performs exploratory testing",
      "3.1  Appropriate tools/frameworks in place to perform automation",
      "3.2  SOLID and DRY principles followed for automation coding",
      "3.3  Test code is reviewed regularly by team members",
      "3.4  Test code is refactored regularly to ensure maintainability",
      "3.5  Test code is changed with changing test cases",
      "3.6  Tests are data-driven",
      "3.7  Test data for different types of runs (environments/scenarios) etc. exist",
      "3.8  Test reports are generated automatically with appropriate metrics recorded",
      "3.9  Tests are optimized for time run regularly",
      "3.10  Test suites for smoke testing exist",
      "3.11  Regression test suites have been created",
      "3.12  Tests are integrated with CICD pipelines",
      "3.13  Where do we run these tests & what is the frequency?"
    ],
    "TOSAssignmentMaturityScoreDesc": ["Additionally, the test automation strategy is devised based on the unit test coverage. QAs share feedback on the unit tests with the developers. The team collectively adheres to the test pyramid.",
      "Dev box testing is an essential part of the testing process. Code coverage tools are implemented, and code coverage is actively looked at, and improved. Code reviews are a regular practice in the team.",
      "The team has regular dev box testing, the code coverage tools are in place, and unit test code reviews are done",
      "The team performs dev box testing only for a few stories, and inconsistently. No code coverage tools in place, and no code reviews.",
      "The team doesn't have any of these practices in place.",
      "Regression tests are constantly upgraded, most times along side or before the product functionality is developed. Automated regression is exhaustive and robust. Team depends on the automated suite for regression cycles. Team spends quality time on exploratory, NFR testing. Defect leakage to production is very rare.",
      "Regression tests are mapped to current functionality and are constantly upgraded based on the change in functionality. The majority of the tests are automated, and the team performs exploratory testing. Some of the automation tests are unreliable and need maintenance. These areas are known and covered using manual testing. The defect leakages to production are very rare. ",
      "Regression tests are well documented, and cover the key functionality of the product/platform. Testing is a combination of manual and automated scripts. Automation tests need maintaince and upgrades. There are few defect leakages to the production environment.",
      "Most of the testing is manual, and tests are documented against the functionality. However, tests are not updated regularly with changes in functionality. Some defects escape the testing cycle and make it to production. There are gaps in regression testing cycles.",
      "All the testing is manual and unstructured. Test cases are not documented and mapped to functional areas. Many defects escape the testing cycle. Regression tests are incomplete, and the cycle is long and ineffective.",
      "Additionally, the team has automated security checks and performance testing. The tests are constantly upgraded and optimized.",
      "Exhaustive automation test coverage is present for most of the regression suite. The test suite is robust and trustworthy. Build times are optimized, and the team depends on the results for build verification. The team spends most of the time on exploratory testing. There is good adherence to the test pyramid.",
      "Key test cases are automated and run on CI. The tests can be made more robust and build times can be optimized. The team fixes the key tests in a timely manner, and the results are analysed for every failure of build and fixed. ",
      "Automated tests are minimal and the team depends on manual testing for regression. Automated tests are flaky and need maintenance. There is no CI in place.",
      "There are no automated tests in place."
    ]
  }
]



